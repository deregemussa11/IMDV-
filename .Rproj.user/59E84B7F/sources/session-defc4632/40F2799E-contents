---
title: "IMDB Movie Reviews Sentiment analysis"
author: ""
date: "2023-06-13"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

## Overview of the project

Project Description:
In this project, I will use the IMDb movie review dataset to perform sentiment analysis on movie reviews. Sentiment analysis involves determining the sentiment or opinion expressed in a piece of text, in this case, movie reviews. By analyzing the sentiments associated with reviews, we can gain insights into the overall reception of movies and potentially predict audience reactions. The dataset is available at kaggle website

## Load required libraries

```{r}
library(tm) # topic modeling
library(e1071) # build model
library(gmodels) # cross tabulation
library(dplyr) # data manipulation
library(readr) # read data
library(tidytext)
library(SnowballC)
library(ggplot2)
```

## Import dataset 

```{r}
movie_reviews <- read_csv("IMDB Dataset.csv")

```

## Explore the dataset

```{r}
glimpse(movie_reviews)

# see how is distribution of the sentiment
movie_reviews |> 
  count(sentiment)
```

The data set has two columns and 50k observations

Tokenization

```{r}

review_df <- movie_reviews |> 
 # select(review) |> 
  unnest_tokens(word, review)

head(review_df)

review_df <- review_df |> 
  mutate(stem = wordStem(word))

review_df <- review_df |> 
  anti_join(stop_words[stop_words$lexicon =='snowball',],by = "word")


```

```{r}

review_df |> 
  group_by(word) |> 
  summarise(count = n()) |> 
  arrange(desc(count)) |> 
  head()

# visualize 

review_df |> 
  count(word, sort = TRUE) |> 
  mutate(word = reorder(word,n)) |> 
  top_n(10) |> 
  ggplot(aes(x = n, y = word))+
  geom_col()+
  labs(title = "Top 10 words in the review")
```

### Prepare for model building 
As I mentioned earlier the data has 50k observation, which relatively large data set to train the model on my personal computer and I will take random sample of 15k. This result will remain the same with `set.seed` parameter for the one who wants to run this script.
```{r}
# convert the review texts into corpus
set.seed(123)
movie_reviews <- sample_n(movie_reviews, size = 15000)
review_corpus <- VCorpus(
VectorSource(movie_reviews$review))
print(review_corpus)

# check actual corpus in the text
as.character(review_corpus[[5]])
```

#### data preparation for the model

```{r}
# clean the text 
movie_revew_clean <- tm_map(review_corpus, content_transformer(tolower))
movie_revew_clean <- tm_map(movie_revew_clean, removeNumbers)
movie_revew_clean <- tm_map(movie_revew_clean, removeWords, stopwords())
movie_revew_clean <- tm_map(movie_revew_clean, removePunctuation)
movie_revew_clean <- tm_map(movie_revew_clean, stripWhitespace)
```

```{r}

as.character(movie_revew_clean[[5]])
# Tokenize as document term matrix
movie_revew_dtm <- DocumentTermMatrix(movie_revew_clean)
```

#### Split the data set 

```{r}
# split the data set to train the model 
movie_reviews_train <- movie_revew_dtm[1:11250,] # 75%
movie_reviews_test <- movie_revew_dtm[11251:15000,] # 25%

movie_reviews_train_labels <- movie_reviews[1:11250,]$sentiment # 75%
movie_reviews_test_labels <- movie_reviews[11251:15000,]$sentiment# 25%

# check the distribution of train and test data 

prop.table(table(movie_reviews_train_labels))
prop.table(table(movie_reviews_test_labels))

# find terms that appear 5 times or more in the review
review_freq_words_tr <- findFreqTerms(movie_reviews_train, 5)
review_freq_words_ts <- findFreqTerms(movie_reviews_test, 5)

# include only the frequent terms in the training and test data
movie_review_freq_train <- movie_reviews_train[, review_freq_words_tr]
movie_review_freq_test <- movie_reviews_test[, review_freq_words_ts]

# function that will check the presence of the each term
check_term <- function(x){
  x <- ifelse(x> 0, "Yes", "No")
}

review_train <- apply(movie_review_freq_train, MARGIN = 2, check_term)
review_test <- apply(movie_review_freq_test, MARGIN = 2, check_term)

```
### Build Model
I will use the Naive Bayes algorithm for this task. Naive Bayes model has some cool advanateges like:
aive Bayes classifiers have several advantages for text classification tasks. Here are some of the key advantages:

Simplicity and Speed: Naive Bayes is a simple and fast algorithm that is computationally efficient. It scales well with large datasets and high-dimensional feature spaces, making it suitable for real-time or online applications.

Ease of Implementation: Naive Bayes classifiers are straightforward to implement and require minimal tuning of hyper parameters. They have a relatively low complexity compared to more complex machine learning algorithms, making them easy to understand and interpret.

Good Performance with Small Training Sets: Naive Bayes can perform well even with small training datasets. It works reasonably well in cases where the number of training examples is limited, making it useful when data availability is a constraint.

```{r}
review_model <- naiveBayes(review_train, movie_reviews_train_labels)
review_test_pred <- predict(review_model, review_test)

# create confusion matrix to see the accuracy of the model
CrossTable(review_test_pred,movie_reviews_test_labels,
           prop.chisq = FALSE,
           prop.t = FALSE, dnn = c("pred", "observed"))
```
Our model is very good with better accuracy of 85% with test dataset. 
